{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsrF0WJo+KyeaXJT1vmJvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21-10-4/zxing/blob/master/RNN%E8%A1%A5%E5%85%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在RNN进行序列样本数据的训练时，会面临序列样本数据长短不一的情况；<br>\n",
        "为了高效地进行batch处理，就需要对样本序列进行填充，保证各个样本长度相同，在 PyTorch 里面使用函数 **pad_sequence** 对序列进行填充。填充之后的样本序列，虽然长度相同了，但是序列里面可能填充了很多无效值 0 ，将填充值 0 喂给 RNN 进行 forward 计算，不仅浪费计算资源，最后得到的值可能还会存在误差。因此在将序列送给 RNN 进行处理之前，需要采用 **pack_padded_sequence** 进行压缩，压缩掉无效的填充值。序列经过 RNN 处理之后的输出仍然是压紧的序列，需要采用 **pad_packed_sequence** 把压紧的序列再填充回来，便于进行后续的处理。<br>\n",
        "https://zhuanlan.zhihu.com/p/342685890"
      ],
      "metadata": {
        "id": "UGCJXLYDRpK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3-LRHJZBRhyg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: len(x), reverse=True)\n",
        "    # print('data_size', len(data))# 2 \n",
        "    # seq_len = [s.size(0) for s in data] # 获取每个样本真实的长度\n",
        "    # data = pad_sequence(data, batch_first=True)    \n",
        "    # data = pack_padded_sequence(data, seq_len, batch_first=True)\n",
        "    data = pack_sequence(data)\n",
        "    return data\n",
        "\n",
        "a = torch.tensor([1,2,3,4])\n",
        "b = torch.tensor([5,6,7])\n",
        "c = torch.tensor([7,8])\n",
        "d = torch.tensor([9])\n",
        "train_x = [a, b, c, d]\n",
        "\n",
        "data = MyData(train_x)\n",
        "data_loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "# 采用默认的 collate_fn 会报错\n",
        "#data_loader = DataLoader(data, batch_size=2, shuffle=True) \n",
        "batch_x = next(iter(data_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "elkDQfiXD3Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRoHs32MSetz",
        "outputId": "fb164c51-3402-4505-a469-4b9edcd537f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([7, 9, 8]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad_packed_sequence 是pack_padded_sequence的逆向操作\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: len(x), reverse=True)\n",
        "    seq_len = [s.size(0) for s in data]\n",
        "    data = pad_sequence(data, batch_first=True).float()    \n",
        "    data = data.unsqueeze(-1) # 升维==>torch.Size([2, 3, 1])\n",
        "    print(data)\n",
        "    print(data.shape)\n",
        "    data = pack_padded_sequence(data, seq_len, batch_first=True)\n",
        "    return data\n",
        "\n",
        "a = torch.tensor([1,2,3,4])\n",
        "b = torch.tensor([5,6,7,9])\n",
        "c = torch.tensor([7,8])\n",
        "d = torch.tensor([9])\n",
        "train_x = [a, b, c, d]\n",
        "\n",
        "data = MyData(train_x)\n",
        "data_loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "batch_x = next(iter(data_loader))\n",
        "\n",
        "# 4为hidden_size 2为batch_size 1，2,4中的1为num_layers\n",
        "# 1,4,1中的第一个1为input_size(the number of expected features in the input)\n",
        "rnn = torch.nn.LSTM(1, 4, 1, batch_first=True)\n",
        "h0 = torch.rand(1, 2, 4).float()\n",
        "c0 = torch.rand(1, 2, 4).float()\n",
        "out, (h1, c1) = rnn(batch_x, (h0, c0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH__olMBSglY",
        "outputId": "95a453a3-ed57-4ec2-e550-481a33fca0a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[5.],\n",
            "         [6.],\n",
            "         [7.],\n",
            "         [9.]],\n",
            "\n",
            "        [[7.],\n",
            "         [8.],\n",
            "         [0.],\n",
            "         [0.]]])\n",
            "torch.Size([2, 4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRgn8aCIaE7G",
        "outputId": "ea878d2b-ad11-41f5-e70a-3f96c9a77c21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[5.],\n",
              "        [7.],\n",
              "        [6.],\n",
              "        [8.],\n",
              "        [7.],\n",
              "        [9.]]), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# out是一个packedSequence类型的对象\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzGtUGbUWC1M",
        "outputId": "ad7ff1a5-c896-45b9-dfc2-95081e88d102"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[ 0.6690, -0.0498,  0.4809,  0.0732],\n",
              "        [ 0.6606,  0.0199,  0.3175,  0.0410],\n",
              "        [ 0.6636, -0.0623,  0.3471,  0.0590],\n",
              "        [ 0.6950, -0.0382,  0.2624,  0.0530],\n",
              "        [ 0.6893, -0.0551,  0.3266,  0.0404],\n",
              "        [ 0.7230, -0.0345,  0.3220,  0.0303]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_pad, out_len = pad_packed_sequence(out, batch_first=True)\n",
        "print(out_pad)\n",
        "print(out_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW2cVcFMWoVu",
        "outputId": "afd6277f-4847-4042-db23-e3fde8e71550"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6690, -0.0498,  0.4809,  0.0732],\n",
            "         [ 0.6636, -0.0623,  0.3471,  0.0590],\n",
            "         [ 0.6893, -0.0551,  0.3266,  0.0404],\n",
            "         [ 0.7230, -0.0345,  0.3220,  0.0303]],\n",
            "\n",
            "        [[ 0.6606,  0.0199,  0.3175,  0.0410],\n",
            "         [ 0.6950, -0.0382,  0.2624,  0.0530],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>)\n",
            "tensor([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_pad.shape # torch.Size([2, 4, 4])分别为batch_size,sequence length,hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRHavGfVW1Ae",
        "outputId": "759871bc-9c27-418b-ca2f-b2228330ca3f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "d1j9f-BHl6X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py \n",
        "# RNN\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "seZnvIoHbeth"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[0].shape #torch.Size([100, 1, 28, 28])\n",
        "next(iter(train_loader))[1].shape #torch.Size([100]) labels "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpFGS9LhemUF",
        "outputId": "88963298-c2cd-42c5-fa93-089de435ca8a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent neural network (many-to-one)\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        # x.size(0)为batch_size\n",
        "        # print(\"ccc\",x.size(0)) 100\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smz4fO6dekGs",
        "outputId": "9d46f241-a248-43e3-8e15-12b53fc40d85"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.7088\n",
            "Epoch [1/2], Step [200/600], Loss: 0.4004\n",
            "Epoch [1/2], Step [300/600], Loss: 0.2306\n",
            "Epoch [1/2], Step [400/600], Loss: 0.2306\n",
            "Epoch [1/2], Step [500/600], Loss: 0.1495\n",
            "Epoch [1/2], Step [600/600], Loss: 0.1160\n",
            "Epoch [2/2], Step [100/600], Loss: 0.0939\n",
            "Epoch [2/2], Step [200/600], Loss: 0.1471\n",
            "Epoch [2/2], Step [300/600], Loss: 0.1727\n",
            "Epoch [2/2], Step [400/600], Loss: 0.2179\n",
            "Epoch [2/2], Step [500/600], Loss: 0.0457\n",
            "Epoch [2/2], Step [600/600], Loss: 0.0669\n",
            "Test Accuracy of the model on the 10000 test images: 97.42 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BRNN"
      ],
      "metadata": {
        "id": "Dy5VFXKhl-O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 2\n",
        "learning_rate = 0.003\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "WqI_DkQ3hstB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Bidirectional recurrent neural network (many-to-one)\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial states\n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "qSrn59DImEee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# language_model"
      ],
      "metadata": {
        "id": "CyFq9zFwCyfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some part of the code was referenced from below.\n",
        "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from data_utils import Dictionary, Corpus\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "num_epochs = 5\n",
        "num_samples = 1000     # number of words to be sampled\n",
        "batch_size = 20\n",
        "seq_length = 30\n",
        "learning_rate = 0.002\n",
        "\n",
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length\n",
        "\n",
        "\n",
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)\n",
        "\n",
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] \n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
        "\n",
        "# Save the model checkpoints\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "xlxBX4pVC1GR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}